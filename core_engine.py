# core_engine.py
# ë²”ê³µì¸ Pro v24 Enterprise - Core Data Engine Module (v24.28.1)
# Feature: Negative Floor Support & Robust Regex Sanitization

import streamlit as st
import pandas as pd
from streamlit_gsheets import GSheetsConnection
import urllib.parse
import time
import uuid
import re
import traceback
import math
from datetime import datetime
import requests
import json

# ==============================================================================
# [SECTION 1: GLOBAL CONFIGURATION]
# ==============================================================================

SHEET_URL = "https://docs.google.com/spreadsheets/d/1bmTnLu-vMvlAGRSsCI4a8lk00U38covWl5Wfn9JZYVU"

SHEET_GIDS = {
    "ì„ëŒ€": "2063575964", "ì„ëŒ€(ì¢…ë£Œ)": "791354475", 
    "ë§¤ë§¤": "1833762712", "ë§¤ë§¤(ì¢…ë£Œ)": "1597438389",
    "ì„ëŒ€ë¸Œë¦¬í•‘": "982780192", "ë§¤ë§¤ë¸Œë¦¬í•‘": "807085458"
}
SHEET_NAMES = list(SHEET_GIDS.keys())

NUMERIC_COLS = ["ë³´ì¦ê¸ˆ", "ì›”ì°¨ì„", "ê¶Œë¦¬ê¸ˆ", "ê´€ë¦¬ë¹„", "ë§¤ë§¤ê°€", "ìˆ˜ìµë¥ ", "ë©´ì ", "ëŒ€ì§€ë©´ì ", "ì—°ë©´ì ", "ì¸µ"]
STRING_COLS = ["êµ¬ë¶„", "ì§€ì—­_êµ¬", "ì§€ì—­_ë™", "ë²ˆì§€", "ê±´ë¬¼ëª…", "ë‚´ìš©", "ë¹„ê³ "]
REQUIRED_COLS = ["ë²ˆì§€"]

# ==============================================================================
# [SECTION 2: DATA SANITIZATION ENGINE]
# ==============================================================================

def normalize_headers(df):
    """
    êµ¬ê¸€ ì‹œíŠ¸ í—¤ë”ë¥¼ í‘œì¤€í™”í•©ë‹ˆë‹¤.
    ë™ì˜ì–´ ì‚¬ì „ì„ í™•ì¥í•˜ì—¬ ì‹¤ë¬´ ìš©ì–´ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
    """
    df.columns = df.columns.str.replace(' ', '').str.strip()
    synonym_map = {
        "ë³´ì¦ê¸ˆ": ["ë³´ì¦ê¸ˆ(ë§Œì›)", "ê¸°ë³´ì¦ê¸ˆ(ë§Œì›)", "ê¸°ë³´ì¦ê¸ˆ", "ë³´ì¦ê¸ˆ", "ë³´ì¦"],
        "ì›”ì°¨ì„": ["ì›”ì°¨ì„(ë§Œì›)", "ê¸°ì›”ì„¸(ë§Œì›)", "ì›”ì„¸(ë§Œì›)", "ì›”ì„¸", "ê¸°ì›”ì„¸", "ì°¨ì„"],
        "ê¶Œë¦¬ê¸ˆ": ["ê¶Œë¦¬ê¸ˆ_ì…ê¸ˆê°€(ë§Œì›)", "ê¶Œë¦¬ê¸ˆ(ë§Œì›)", "ê¶Œë¦¬ê¸ˆ", "ê¶Œë¦¬", "ì‹œì„¤ê¶Œë¦¬"],
        "ê´€ë¦¬ë¹„": ["ê´€ë¦¬ë¹„(ë§Œì›)", "ê´€ë¦¬ë¹„"],
        "ë§¤ë§¤ê°€": ["ë§¤ë§¤ê°€(ë§Œì›)", "ë§¤ë§¤ê¸ˆì•¡(ë§Œì›)", "ë§¤ë§¤ê¸ˆì•¡", "ë§¤ë§¤ê°€", "ë§¤ê°€", "ë§¤ë§¤"],
        "ë©´ì ": ["ì „ìš©ë©´ì (í‰)", "ì‹¤í‰ìˆ˜", "ì „ìš©ë©´ì ", "ë©´ì "],
        "ëŒ€ì§€ë©´ì ": ["ëŒ€ì§€ë©´ì (í‰)", "ëŒ€ì§€", "ëŒ€ì§€ë©´ì "],
        "ì—°ë©´ì ": ["ì—°ë©´ì (í‰)", "ì—°ë©´ì "],
        "ìˆ˜ìµë¥ ": ["ìˆ˜ìµë¥ (%)", "ìˆ˜ìµë¥ "],
        "ì¸µ": ["í•´ë‹¹ì¸µ", "ì¸µ", "ì§€ìƒì¸µ", "ì¸µìˆ˜"],
        "ë‚´ìš©": ["ë§¤ë¬¼íŠ¹ì§•", "íŠ¹ì§•", "ë¹„ê³ ", "ë‚´ìš©"],
        "ë²ˆì§€": ["ì§€ì—­_ë²ˆì§€", "ë²ˆì§€", "ì§€ë²ˆ"],
        "êµ¬ë¶„": ["ë§¤ë¬¼êµ¬ë¶„", "êµ¬ë¶„"],
        "ê±´ë¬¼ëª…": ["ê±´ë¬¼ëª…", "ë¹Œë”©ëª…"]
    }
    for standard, aliases in synonym_map.items():
        for alias in aliases:
            clean_alias = alias.replace(' ', '')
            if clean_alias in df.columns:
                df.rename(columns={clean_alias: standard}, inplace=True)
                break 
    return df

def sanitize_dataframe(df):
    """
    ë°ì´í„°í”„ë ˆì„ ê°’ì„ ì •ì œí•©ë‹ˆë‹¤.
    [v24.28.1] ì¸µ ì»¬ëŸ¼ì€ ë§ˆì´ë„ˆìŠ¤(-) ê¸°í˜¸ë¥¼ ë³´ì¡´í•˜ì—¬ ì§€í•˜ì¸µì„ ì˜¬ë°”ë¥´ê²Œ ì¸ì‹í•©ë‹ˆë‹¤.
    ë‚˜ë¨¸ì§€ ìˆ«ì ì»¬ëŸ¼ì€ ìˆ«ìì™€ ì†Œìˆ˜ì ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    """
    for col in NUMERIC_COLS:
        if col in df.columns:
            try:
                val_str = df[col].astype(str)
                
                if col == 'ì¸µ':
                    # [ì§€í•˜ì¸µ ë³µêµ¬ ë¡œì§] ìŒìˆ˜ ê¸°í˜¸(-)ì™€ ìˆ«ì, ì†Œìˆ˜ì  ì¶”ì¶œ
                    # extractë¥¼ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ ìˆ«ì íŒ¨í„´(ìŒìˆ˜ í¬í•¨)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    cleaned_series = val_str.str.extract(r'(-?[\d.]+)')[0]
                else:
                    # [ì¼ë°˜ ìˆ«ì ë¡œì§] ìˆ«ì(0-9)ì™€ ì†Œìˆ˜ì (.)ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°
                    cleaned_series = val_str.str.replace(r'[^0-9.]', '', regex=True)
                
                # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬ ë° ìˆ«ì ë³€í™˜
                df[col] = pd.to_numeric(cleaned_series, errors='coerce').fillna(0)
            except: 
                df[col] = 0.0
                
    for col in STRING_COLS:
        if col in df.columns:
            try:
                # ë¬¸ìì—´ ì»¬ëŸ¼: ë¶ˆí•„ìš”í•œ ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
                df[col] = df[col].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip()
            except: 
                df[col] = ""
                
    return df.fillna("")

def validate_data_integrity(df):
    """
    í•„ìˆ˜ ì»¬ëŸ¼ ë° ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    errors = []
    for col in REQUIRED_COLS:
        if col not in df.columns: 
            errors.append(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")
        elif df[col].astype(str).str.strip().eq("").any():
            errors.append(f"í•„ìˆ˜ê°’ ëˆ„ë½: {col} ì»¬ëŸ¼ì— ë¹ˆ í–‰ì´ ìˆìŠµë‹ˆë‹¤.")
    
    if errors: 
        return False, "\n".join(errors)
    return True, "Integrity Check Passed"

# ==============================================================================
# [SECTION 3: CORE LOAD ENGINE]
# ==============================================================================

def initialize_search_state():
    """
    ì•± ì‹¤í–‰ ì‹œ ì„¸ì…˜ ìƒíƒœ(ê²€ìƒ‰ í•„í„° ë“±)ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    if 'editor_key_version' not in st.session_state:
        st.session_state.editor_key_version = 0
        
    defaults = {
        'search_keyword': "", 'exact_bunji': "", 'selected_cat': [], 
        'selected_gu': [], 'selected_dong': [], 'is_no_kwon': False,
        'min_price': 0.0, 'max_price': 10000000.0, 'min_dep': 0.0, 'max_dep': 1000000.0,
        'min_rent': 0.0, 'max_rent': 10000.0, 'min_kwon': 0.0, 'max_kwon': 100000.0,
        'min_area': 0.0, 'max_area': 10000.0, 'min_land': 0.0, 'max_land': 10000.0,
        'min_total': 0.0, 'max_total': 10000.0, 'min_fl': -5.0, 'max_fl': 100.0 # ì¸µ ë²”ìœ„ í™•ì¥
    }
    for k, v in defaults.items():
        if k not in st.session_state: st.session_state[k] = v

def safe_reset():
    """
    í•„í„° ê´€ë ¨ ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ê³  ë¦¬ëŸ°í•©ë‹ˆë‹¤.
    """
    for key in list(st.session_state.keys()):
        # í•µì‹¬ ì‹œìŠ¤í…œ ë³€ìˆ˜ëŠ” ìœ ì§€
        if key not in ['current_sheet', 'editor_key_version']:
            del st.session_state[key]
    st.session_state.editor_key_version += 1
    st.cache_data.clear()
    st.rerun()

@st.cache_data(ttl=600)
def load_sheet_data(sheet_name):
    """
    êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    gid = SHEET_GIDS.get(sheet_name)
    if not gid: return None
    
    csv_url = f"{SHEET_URL}/export?format=csv&gid={gid}"
    
    try:
        df = pd.read_csv(csv_url)
        df = normalize_headers(df)
        df = sanitize_dataframe(df)
        
        # ì‹œìŠ¤í…œ ì»¬ëŸ¼ ì œê±°
        drop_cols = [c for c in ['ì„ íƒ', 'IronID', 'Unnamed: 0'] if c in df.columns]
        df = df.drop(columns=drop_cols, errors='ignore')
        
        # ë¡œì»¬ ì‹ë³„ì(IronID) ë° ì„ íƒ ì»¬ëŸ¼ ì¶”ê°€
        df['IronID'] = [str(uuid.uuid4()) for _ in range(len(df))]
        df.insert(0, 'ì„ íƒ', False)
        
        return df
    except Exception as e:
        print(f"[Load Error] {e}")
        return None

# ==============================================================================
# [SECTION 4: MATCHING ENGINE]
# ==============================================================================

def create_match_signature(df, keys):
    """
    ë°ì´í„° ë§¤ì¹­ì„ ìœ„í•œ ê³ ìœ  ì„œëª…(Signature)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    [v24.28.1] ì¸µìˆ˜ì˜ ìŒìˆ˜ê°’ì„ ë³´ì¡´í•˜ì—¬ ì„œëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    temp_df = df.copy()
    temp_df['_match_sig'] = ""
    
    for k in keys:
        try:
            if k in NUMERIC_COLS:
                val_str = temp_df[k].astype(str)
                if k == 'ì¸µ':
                    # ì¸µìˆ˜ëŠ” ìŒìˆ˜ í¬í•¨ ì¶”ì¶œ
                    val_str = val_str.str.extract(r'(-?[\d.]+)')[0]
                else:
                    # ë‚˜ë¨¸ì§€ëŠ” ìˆ«ìë§Œ
                    val_str = val_str.str.replace(r'[^0-9.]', '', regex=True)
                
                val = pd.to_numeric(val_str, errors='coerce').fillna(0)
                temp_df['_match_sig'] += val.round(1).astype(str).str.replace(r'\.0$', '', regex=True) + "|"
            else:
                # ë¬¸ìí˜•: íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì• 20ê¸€ìë§Œ ì‚¬ìš©
                val = temp_df[k].astype(str).str[:20] if k == 'ë‚´ìš©' else temp_df[k].astype(str)
                temp_df['_match_sig'] += val.str.replace(r'[^ê°€-í£a-zA-Z0-9]', '', regex=True) + "|"
        except: continue
        
    return temp_df

# ==============================================================================
# [SECTION 5: UPDATE ENGINE (FULL LOGIC)]
# ==============================================================================

def update_single_row(updated_row, sheet_name):
    """
    [Phase 4] ìƒì„¸ ë³´ê¸° í™”ë©´ì—ì„œ ë‹¨ì¼ í–‰ì„ ì¦‰ì‹œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    [v24.28.1] ì¸µ ìˆ˜ì • ì‹œ ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€.
    """
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        # 1. ì„œë²„ ë°ì´í„° ë¡œë“œ
        sheet_data = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=sheet_name, ttl=0))
        
        # 2. ë§¤ì¹­ í‚¤ ì„¤ì •
        match_keys = ['ë²ˆì§€', 'ì¸µ', 'ë©´ì '] 
        valid_keys = [k for k in match_keys if k in sheet_data.columns and k in updated_row]
        
        if len(valid_keys) < 2: 
            return False, "ì‹ë³„ í‚¤ ë¶€ì¡± (ë²ˆì§€/ì¸µ/ë©´ì  í•„ìˆ˜)"
        
        # 3. ë¡œì»¬ ë°ì´í„° ì„œëª… ìƒì„± (ì •ê·œì‹ ì´ì›í™”)
        local_sig = ""
        for k in valid_keys:
             val_str = str(updated_row.get(k, ''))
             if k in NUMERIC_COLS:
                 if k == 'ì¸µ':
                     match = re.search(r'(-?[\d.]+)', val_str)
                     val_clean = match.group(1) if match else "0"
                 else:
                     val_clean = re.sub(r'[^0-9.]', '', val_str)
                 
                 try: val = str(round(float(val_clean), 1)).replace('.0', '')
                 except: val = "0"
             else:
                 val = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', val_str)
             local_sig += val + "|"
             
        # 4. ì„œë²„ ë°ì´í„° ì„œëª… ìƒì„± (ì •ê·œì‹ ì´ì›í™”)
        server_sigs = []
        for _, row in sheet_data.iterrows():
            sig = ""
            for k in valid_keys:
                val_str = str(row.get(k, ''))
                if k in NUMERIC_COLS:
                    if k == 'ì¸µ':
                        match = re.search(r'(-?[\d.]+)', val_str)
                        val_clean = match.group(1) if match else "0"
                    else:
                        val_clean = re.sub(r'[^0-9.]', '', val_str)
                        
                    try: val = str(round(float(val_clean), 1)).replace('.0', '')
                    except: val = "0"
                else:
                    val = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', val_str)
                sig += val + "|"
            server_sigs.append(sig)
            
        # 5. ë§¤ì¹­ ë° ì—…ë°ì´íŠ¸
        try:
            target_idx = server_sigs.index(local_sig)
            
            # ê°’ ë®ì–´ì“°ê¸°
            for k, v in updated_row.items():
                if k in sheet_data.columns and k not in ['ì„ íƒ', 'IronID']:
                    if k in NUMERIC_COLS:
                        try: 
                            raw_v = str(v)
                            if k == 'ì¸µ':
                                # ì¸µìˆ˜ëŠ” ë§ˆì´ë„ˆìŠ¤ í—ˆìš©
                                match = re.search(r'(-?[\d.]+)', raw_v)
                                v_str = match.group(1) if match else "0"
                            else:
                                # ì¼ë°˜ ìˆ«ìëŠ” ë§ˆì´ë„ˆìŠ¤ ì œê±°
                                v_str = re.sub(r'[^0-9.]', '', raw_v)
                            
                            v = float(v_str) if v_str else 0.0
                        except: v = 0.0
                    sheet_data.at[target_idx, k] = v
            
            # 6. ì €ì¥
            conn.update(spreadsheet=SHEET_URL, worksheet=sheet_name, data=sheet_data)
            return True, "âœ… ìˆ˜ì • ì‚¬í•­ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
            
        except ValueError:
            return False, "âŒ ì›ë³¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í‚¤ ê°’ì´ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŒ)"
            
    except Exception as e:
        return False, f"ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"

def save_updates_to_sheet(edited_df, original_df, sheet_name):
    """
    [Phase 1] ë¦¬ìŠ¤íŠ¸/ì¹´ë“œ ë·°ì—ì„œì˜ ëŒ€ëŸ‰ ìˆ˜ì • ì‚¬í•­ì„ ë°°ì¹˜ ì €ì¥í•©ë‹ˆë‹¤.
    [v24.28.1] ì§€í•˜ì¸µìˆ˜ ë°ì´í„° ë³´ì¡´ ë¡œì§ ì ìš©.
    """
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        # 1. ë³€ê²½ëœ í–‰ ê°ì§€
        df_org = original_df.set_index('IronID')
        df_new = edited_df.set_index('IronID')
        
        changed_ids = []
        for iid in df_org.index.intersection(df_new.index):
            row_org = df_org.loc[iid].drop(['ì„ íƒ'], errors='ignore').astype(str)
            row_new = df_new.loc[iid].drop(['ì„ íƒ'], errors='ignore').astype(str)
            if not row_org.equals(row_new):
                changed_ids.append(iid)
        
        if not changed_ids: return True, "ë³€ê²½ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.", None

        # 2. ì¬ì‹œë„ ë¡œì§
        for attempt in range(3):
            try:
                sheet_data = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=sheet_name, ttl=0))
                valid_keys = [k for k in ['ë²ˆì§€', 'ì¸µ', 'ë©´ì ', 'ë§¤ë§¤ê°€', 'ë³´ì¦ê¸ˆ'] if k in sheet_data.columns and k in df_org.columns]
                if len(valid_keys) < 2: return False, "ì‹ë³„ í‚¤ ë¶€ì¡±", None

                # ë§¤ì¹­ ì„œëª… ìƒì„± ì‹œì—ë„ ì¸µìˆ˜ëŠ” ìŒìˆ˜ í—ˆìš© ë¡œì§ì´ create_match_signatureì— ë°˜ì˜ë¨
                target_sigs = create_match_signature(df_org.loc[changed_ids].reset_index(), valid_keys)['_match_sig'].tolist()
                server_sigs = create_match_signature(sheet_data, valid_keys)
                
                update_count = 0
                for idx, sig in zip(target_sigs, changed_ids):
                    match_indices = server_sigs.index[server_sigs['_match_sig'] == idx].tolist()
                    if match_indices:
                        match_idx = match_indices[0]
                        for col in sheet_data.columns:
                            if col in df_new.columns: 
                                val = df_new.loc[sig, col]
                                if col in NUMERIC_COLS:
                                    try: 
                                        val_str = str(val)
                                        if col == 'ì¸µ':
                                            # ì¸µì¼ ê²½ìš° ìŒìˆ˜ íŒ¨í„´ ì¶”ì¶œ
                                            match = re.search(r'(-?[\d.]+)', val_str)
                                            val_clean = match.group(1) if match else "0"
                                        else:
                                            # ë‚˜ë¨¸ì§€ëŠ” ìˆ«ìë§Œ
                                            val_clean = re.sub(r'[^0-9.]', '', val_str)
                                            
                                        val = float(val_clean) if val_clean else 0.0
                                    except: val = 0.0
                                sheet_data.at[match_idx, col] = val
                        update_count += 1
                
                if update_count == 0: return False, "ì›ë³¸ ë°ì´í„° ë§¤ì¹­ ì‹¤íŒ¨", None
                
                is_valid, msg = validate_data_integrity(sheet_data)
                if not is_valid: return False, f"ë¬´ê²°ì„± ì˜¤ë¥˜: {msg}", None
                
                conn.update(spreadsheet=SHEET_URL, worksheet=sheet_name, data=sheet_data)
                return True, f"âœ… {update_count}ê±´ ì €ì¥ ì™„ë£Œ!", None
                
            except Exception as e:
                time.sleep(attempt + 1)
                last_err = e
                continue
                
        return False, f"ğŸš¨ ì¬ì‹œë„ ì‹¤íŒ¨: {last_err}", None
        
    except Exception as e: 
        return False, f"ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", traceback.format_exc()

def execute_transaction(action_type, target_rows, source_sheet, target_sheet=None):
    """
    [Phase 2] ì‚­ì œ, ì´ë™, ë³µêµ¬, ë³µì‚¬ íŠ¸ëœì­ì…˜ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        if target_rows.empty: return False, "ëŒ€ìƒ ì—†ìŒ", None
        
        # 1. ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
        src_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=source_sheet, ttl=0))
        target_clean = target_rows.drop(columns=['ì„ íƒ', 'IronID'], errors='ignore')
        
        # 2. ë§¤ì¹­ í‚¤ ì„¤ì •
        v_keys = [k for k in ['ë²ˆì§€', 'ì¸µ', 'ë©´ì ', 'ë³´ì¦ê¸ˆ', 'ë§¤ë§¤ê°€', 'ì›”ì°¨ì„', 'ë‚´ìš©'] if k in src_df.columns and k in target_clean.columns]
        
        # 3. ì„œëª… ìƒì„±
        src_sig = create_match_signature(src_df, v_keys)
        tgt_sig = create_match_signature(target_clean, v_keys)
        sigs = tgt_sig['_match_sig'].tolist()

        # 4. ì•¡ì…˜ ë¶„ê¸°
        if action_type in ["delete", "move", "restore"]:
            # ì†ŒìŠ¤ì—ì„œ ì œê±°
            new_src = src_df[~src_sig['_match_sig'].isin(sigs)]
            
            if len(src_df) == len(new_src): 
                return False, "ë§¤ì¹­ ì‹¤íŒ¨ (ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ë³€ê²½ë¨)", pd.DataFrame({"Target": sigs[:1], "Server": src_sig['_match_sig'].iloc[:1]})
            
            # ì´ë™/ë³µêµ¬: íƒ€ê²Ÿ ì‹œíŠ¸ì— ì¶”ê°€
            if action_type in ["move", "restore"] and target_sheet:
                tgt_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=target_sheet, ttl=0))
                common_cols = [c for c in src_df.columns if c in tgt_df.columns]
                new_tgt = pd.concat([tgt_df, target_clean[common_cols]], ignore_index=True)
                
                is_valid, msg = validate_data_integrity(new_tgt)
                if not is_valid: return False, msg, None
                
                conn.update(spreadsheet=SHEET_URL, worksheet=target_sheet, data=new_tgt)
            
            # ì†ŒìŠ¤ ì‹œíŠ¸ ì—…ë°ì´íŠ¸
            conn.update(spreadsheet=SHEET_URL, worksheet=source_sheet, data=new_src)
            return True, f"âœ… {action_type} ì²˜ë¦¬ ì™„ë£Œ", None
        
        elif action_type == "copy":
            tgt_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=target_sheet, ttl=0))
            common_cols = [c for c in src_df.columns if c in tgt_df.columns]
            new_tgt = pd.concat([tgt_df, target_clean[common_cols]], ignore_index=True)
            
            conn.update(spreadsheet=SHEET_URL, worksheet=target_sheet, data=new_tgt)
            return True, "âœ… ë³µì‚¬ ì™„ë£Œ", None
            
    except Exception as e: 
        return False, str(e), traceback.format_exc()
