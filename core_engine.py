# core_engine.py
# ë²”ê³µì¸ Pro v24 Enterprise - Core Data Engine Module (v24.24.1)
# Feature: Advanced Header Synonyms & Regex Data Sanitization

import streamlit as st
import pandas as pd
from streamlit_gsheets import GSheetsConnection
import urllib.parse
import time
import uuid
import re
import traceback
import math
from datetime import datetime
import requests
import json

# ==============================================================================
# [SECTION 1: GLOBAL CONFIGURATION]
# ==============================================================================

SHEET_URL = "https://docs.google.com/spreadsheets/d/1bmTnLu-vMvlAGRSsCI4a8lk00U38covWl5Wfn9JZYVU"

SHEET_GIDS = {
    "ì„ëŒ€": "2063575964", "ì„ëŒ€(ì¢…ë£Œ)": "791354475", 
    "ë§¤ë§¤": "1833762712", "ë§¤ë§¤(ì¢…ë£Œ)": "1597438389",
    "ì„ëŒ€ë¸Œë¦¬í•‘": "982780192", "ë§¤ë§¤ë¸Œë¦¬í•‘": "807085458"
}
SHEET_NAMES = list(SHEET_GIDS.keys())

NUMERIC_COLS = ["ë³´ì¦ê¸ˆ", "ì›”ì°¨ì„", "ê¶Œë¦¬ê¸ˆ", "ê´€ë¦¬ë¹„", "ë§¤ë§¤ê°€", "ìˆ˜ìµë¥ ", "ë©´ì ", "ëŒ€ì§€ë©´ì ", "ì—°ë©´ì ", "ì¸µ"]
STRING_COLS = ["êµ¬ë¶„", "ì§€ì—­_êµ¬", "ì§€ì—­_ë™", "ë²ˆì§€", "ê±´ë¬¼ëª…", "ë‚´ìš©", "ë¹„ê³ "]
REQUIRED_COLS = ["ë²ˆì§€"]

# ==============================================================================
# [SECTION 2: DATA SANITIZATION ENGINE (ENHANCED)]
# ==============================================================================

def normalize_headers(df):
    """
    êµ¬ê¸€ ì‹œíŠ¸ í—¤ë”ë¥¼ í‘œì¤€í™”í•©ë‹ˆë‹¤.
    ë™ì˜ì–´ ì‚¬ì „ì„ ëŒ€í­ í™•ì¥í•˜ì—¬ ì‹¤ë¬´ ìš©ì–´ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
    """
    df.columns = df.columns.str.replace(' ', '').str.strip()
    synonym_map = {
        "ë³´ì¦ê¸ˆ": ["ë³´ì¦ê¸ˆ(ë§Œì›)", "ê¸°ë³´ì¦ê¸ˆ(ë§Œì›)", "ê¸°ë³´ì¦ê¸ˆ", "ë³´ì¦ê¸ˆ", "ë³´ì¦"],
        "ì›”ì°¨ì„": ["ì›”ì°¨ì„(ë§Œì›)", "ê¸°ì›”ì„¸(ë§Œì›)", "ì›”ì„¸(ë§Œì›)", "ì›”ì„¸", "ê¸°ì›”ì„¸", "ì°¨ì„"],
        "ê¶Œë¦¬ê¸ˆ": ["ê¶Œë¦¬ê¸ˆ_ì…ê¸ˆê°€(ë§Œì›)", "ê¶Œë¦¬ê¸ˆ(ë§Œì›)", "ê¶Œë¦¬ê¸ˆ"],
        "ê´€ë¦¬ë¹„": ["ê´€ë¦¬ë¹„(ë§Œì›)", "ê´€ë¦¬ë¹„"],
        "ë§¤ë§¤ê°€": ["ë§¤ë§¤ê°€(ë§Œì›)", "ë§¤ë§¤ê¸ˆì•¡(ë§Œì›)", "ë§¤ë§¤ê¸ˆì•¡", "ë§¤ë§¤ê°€", "ë§¤ê°€", "ë§¤ë§¤"],
        "ë©´ì ": ["ì „ìš©ë©´ì (í‰)", "ì‹¤í‰ìˆ˜", "ì „ìš©ë©´ì ", "ë©´ì "],
        "ëŒ€ì§€ë©´ì ": ["ëŒ€ì§€ë©´ì (í‰)", "ëŒ€ì§€", "ëŒ€ì§€ë©´ì "],
        "ì—°ë©´ì ": ["ì—°ë©´ì (í‰)", "ì—°ë©´ì "],
        "ìˆ˜ìµë¥ ": ["ìˆ˜ìµë¥ (%)", "ìˆ˜ìµë¥ "],
        "ì¸µ": ["í•´ë‹¹ì¸µ", "ì¸µ", "ì§€ìƒì¸µ", "ì¸µìˆ˜"],
        "ë‚´ìš©": ["ë§¤ë¬¼íŠ¹ì§•", "íŠ¹ì§•", "ë¹„ê³ ", "ë‚´ìš©"],
        "ë²ˆì§€": ["ì§€ì—­_ë²ˆì§€", "ë²ˆì§€", "ì§€ë²ˆ"],
        "êµ¬ë¶„": ["ë§¤ë¬¼êµ¬ë¶„", "êµ¬ë¶„"],
        "ê±´ë¬¼ëª…": ["ê±´ë¬¼ëª…", "ë¹Œë”©ëª…"]
    }
    for standard, aliases in synonym_map.items():
        for alias in aliases:
            clean_alias = alias.replace(' ', '')
            if clean_alias in df.columns:
                df.rename(columns={clean_alias: standard}, inplace=True)
                break 
    return df

def sanitize_dataframe(df):
    """
    ë°ì´í„°í”„ë ˆì„ ê°’ì„ ê°•ë ¥í•˜ê²Œ ì •ì œí•©ë‹ˆë‹¤.
    [í•µì‹¬ ìˆ˜ì •] ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìˆ«ìí˜• ì»¬ëŸ¼ì—ì„œ 'ë§Œì›', 'í‰', 'ì¸µ' ë“± ë¶ˆìˆœë¬¼ì„ ê°•ì œë¡œ ì œê±°í•©ë‹ˆë‹¤.
    """
    for col in NUMERIC_COLS:
        if col in df.columns:
            try:
                # 1. ë¬¸ìì—´ë¡œ ë³€í™˜
                # 2. ì •ê·œì‹: ìˆ«ì(0-9)ì™€ ì†Œìˆ˜ì (.)ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°
                # 3. ë¹ˆ ë¬¸ìì—´ì´ ë˜ë©´ NaN ì²˜ë¦¬ í›„ 0ìœ¼ë¡œ ì±„ì›€
                cleaned_series = df[col].astype(str).str.replace(r'[^0-9.]', '', regex=True)
                df[col] = pd.to_numeric(cleaned_series, errors='coerce').fillna(0)
            except: 
                df[col] = 0.0
                
    for col in STRING_COLS:
        if col in df.columns:
            try:
                # ë¬¸ìì—´ ì»¬ëŸ¼: ë¶ˆí•„ìš”í•œ ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
                df[col] = df[col].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip()
            except: 
                df[col] = ""
                
    return df.fillna("")

def validate_data_integrity(df):
    """
    í•„ìˆ˜ ì»¬ëŸ¼ ë° ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    errors = []
    for col in REQUIRED_COLS:
        if col not in df.columns: 
            errors.append(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")
        elif df[col].astype(str).str.strip().eq("").any():
            errors.append(f"í•„ìˆ˜ê°’ ëˆ„ë½: {col} ì»¬ëŸ¼ì— ë¹ˆ í–‰ì´ ìˆìŠµë‹ˆë‹¤.")
    
    if errors: 
        return False, "\n".join(errors)
    return True, "Integrity Check Passed"

# ==============================================================================
# [SECTION 3: CORE LOAD ENGINE]
# ==============================================================================

def initialize_search_state():
    """
    ì•± ì‹¤í–‰ ì‹œ ì„¸ì…˜ ìƒíƒœ(ê²€ìƒ‰ í•„í„° ë“±)ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    if 'editor_key_version' not in st.session_state:
        st.session_state.editor_key_version = 0
        
    defaults = {
        'search_keyword': "", 'exact_bunji': "", 'selected_cat': [], 
        'selected_gu': [], 'selected_dong': [], 'is_no_kwon': False,
        'min_price': 0.0, 'max_price': 10000000.0, 'min_dep': 0.0, 'max_dep': 1000000.0,
        'min_rent': 0.0, 'max_rent': 10000.0, 'min_kwon': 0.0, 'max_kwon': 100000.0,
        'min_area': 0.0, 'max_area': 10000.0, 'min_land': 0.0, 'max_land': 10000.0,
        'min_total': 0.0, 'max_total': 10000.0, 'min_fl': -2.0, 'max_fl': 50.0
    }
    for k, v in defaults.items():
        if k not in st.session_state: st.session_state[k] = v

def safe_reset():
    """
    í•„í„° ê´€ë ¨ ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ê³  ë¦¬ëŸ°í•©ë‹ˆë‹¤.
    """
    for key in list(st.session_state.keys()):
        # í•µì‹¬ ì‹œìŠ¤í…œ ë³€ìˆ˜ëŠ” ìœ ì§€
        if key not in ['current_sheet', 'editor_key_version']:
            del st.session_state[key]
    st.session_state.editor_key_version += 1
    st.cache_data.clear()
    st.rerun()

@st.cache_data(ttl=600)
def load_sheet_data(sheet_name):
    """
    êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    gid = SHEET_GIDS.get(sheet_name)
    if not gid: return None
    
    csv_url = f"{SHEET_URL}/export?format=csv&gid={gid}"
    
    try:
        df = pd.read_csv(csv_url)
        df = normalize_headers(df)
        df = sanitize_dataframe(df)
        
        # ì‹œìŠ¤í…œ ì»¬ëŸ¼ ì œê±° (ë¡œë“œ ì‹œ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì •ë¦¬)
        drop_cols = [c for c in ['ì„ íƒ', 'IronID', 'Unnamed: 0'] if c in df.columns]
        df = df.drop(columns=drop_cols, errors='ignore')
        
        # ë¡œì»¬ ì‹ë³„ì(IronID) ë° ì„ íƒ ì»¬ëŸ¼ ì¶”ê°€
        df['IronID'] = [str(uuid.uuid4()) for _ in range(len(df))]
        df.insert(0, 'ì„ íƒ', False)
        
        return df
    except Exception as e:
        print(f"[Load Error] {e}")
        return None

# ==============================================================================
# [SECTION 4: MATCHING ENGINE]
# ==============================================================================

def create_match_signature(df, keys):
    """
    ë°ì´í„° ë§¤ì¹­ì„ ìœ„í•œ ê³ ìœ  ì„œëª…(Signature)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    temp_df = df.copy()
    temp_df['_match_sig'] = ""
    
    for k in keys:
        try:
            if k in NUMERIC_COLS:
                # ìˆ«ìí˜•: ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼ í›„ ë¬¸ìì—´ ë³€í™˜ (ì½¤ë§ˆ ì œê±° í¬í•¨)
                val = pd.to_numeric(temp_df[k].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
                temp_df['_match_sig'] += val.round(1).astype(str).str.replace(r'\.0$', '', regex=True) + "|"
            else:
                # ë¬¸ìí˜•: íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì• 20ê¸€ìë§Œ ì‚¬ìš© (ê¸´ í…ìŠ¤íŠ¸ ì˜¤ì°¨ ë°©ì§€)
                val = temp_df[k].astype(str).str[:20] if k == 'ë‚´ìš©' else temp_df[k].astype(str)
                temp_df['_match_sig'] += val.str.replace(r'[^ê°€-í£a-zA-Z0-9]', '', regex=True) + "|"
        except: continue
        
    return temp_df

# ==============================================================================
# [SECTION 5: UPDATE ENGINE (FULL LOGIC)]
# ==============================================================================

def update_single_row(updated_row, sheet_name):
    """
    [Phase 4] ìƒì„¸ ë³´ê¸° í™”ë©´ì—ì„œ ë‹¨ì¼ í–‰ì„ ì¦‰ì‹œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        # 1. ì„œë²„ ë°ì´í„° ë¡œë“œ (TTL=0: ìµœì‹  ë°ì´í„° ê°•ì œ)
        sheet_data = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=sheet_name, ttl=0))
        
        # 2. ë§¤ì¹­ í‚¤ ì„¤ì • (ë¶ˆë³€ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì»¬ëŸ¼ ì¡°í•©)
        match_keys = ['ë²ˆì§€', 'ì¸µ', 'ë©´ì '] 
        valid_keys = [k for k in match_keys if k in sheet_data.columns and k in updated_row]
        
        if len(valid_keys) < 2: 
            return False, "ì‹ë³„ í‚¤ ë¶€ì¡± (ë²ˆì§€/ì¸µ/ë©´ì  í•„ìˆ˜)"
        
        # 3. ë¡œì»¬ ë°ì´í„° ì„œëª… ìƒì„± (ë‹¨ì¼ í–‰)
        local_sig = ""
        for k in valid_keys:
             val = str(updated_row.get(k, '')).replace(',', '').strip()
             if k in NUMERIC_COLS:
                 try: val = str(round(float(val), 1)).replace('.0', '')
                 except: val = "0"
             else:
                 val = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', val)
             local_sig += val + "|"
             
        # 4. ì„œë²„ ë°ì´í„° ì„œëª… ìƒì„± (ì „ì²´ í–‰)
        server_sigs = []
        for _, row in sheet_data.iterrows():
            sig = ""
            for k in valid_keys:
                val = str(row.get(k, '')).replace(',', '').strip()
                if k in NUMERIC_COLS:
                    try: val = str(round(float(val), 1)).replace('.0', '')
                    except: val = "0"
                else:
                    val = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', val)
                sig += val + "|"
            server_sigs.append(sig)
            
        # 5. ë§¤ì¹­ ë° ì—…ë°ì´íŠ¸
        try:
            target_idx = server_sigs.index(local_sig)
            
            # ê°’ ë®ì–´ì“°ê¸°
            for k, v in updated_row.items():
                if k in sheet_data.columns and k not in ['ì„ íƒ', 'IronID']:
                    # ìˆ«ìí˜• ë°ì´í„° ì•ˆì „ ë³€í™˜
                    if k in NUMERIC_COLS:
                        try: 
                            # ì •ê·œì‹ ì •ì œ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
                            v_str = re.sub(r'[^0-9.]', '', str(v))
                            v = float(v_str) if v_str else 0.0
                        except: v = 0.0
                    sheet_data.at[target_idx, k] = v
            
            # 6. êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥
            conn.update(spreadsheet=SHEET_URL, worksheet=sheet_name, data=sheet_data)
            return True, "âœ… ìˆ˜ì • ì‚¬í•­ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
            
        except ValueError:
            return False, "âŒ ì›ë³¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í‚¤ ê°’ì´ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŒ)"
            
    except Exception as e:
        return False, f"ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"

def save_updates_to_sheet(edited_df, original_df, sheet_name):
    """
    [Phase 1] ë¦¬ìŠ¤íŠ¸/ì¹´ë“œ ë·°ì—ì„œì˜ ëŒ€ëŸ‰ ìˆ˜ì • ì‚¬í•­ì„ ë°°ì¹˜ ì €ì¥í•©ë‹ˆë‹¤.
    """
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        # 1. ë³€ê²½ëœ í–‰ ê°ì§€
        df_org = original_df.set_index('IronID')
        df_new = edited_df.set_index('IronID')
        
        # 'ì„ íƒ' ì»¬ëŸ¼ ì œì™¸í•˜ê³  ë¹„êµ
        changed_ids = []
        for iid in df_org.index.intersection(df_new.index):
            row_org = df_org.loc[iid].drop(['ì„ íƒ'], errors='ignore').astype(str)
            row_new = df_new.loc[iid].drop(['ì„ íƒ'], errors='ignore').astype(str)
            if not row_org.equals(row_new):
                changed_ids.append(iid)
        
        if not changed_ids: return True, "ë³€ê²½ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.", None

        # 2. ì¬ì‹œë„ ë¡œì§ (Optimistic Locking ì‹œë„)
        for attempt in range(3):
            try:
                # ìµœì‹  ì„œë²„ ë°ì´í„° ë¡œë“œ
                sheet_data = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=sheet_name, ttl=0))
                
                # ë§¤ì¹­ í‚¤ ì„¤ì •
                valid_keys = [k for k in ['ë²ˆì§€', 'ì¸µ', 'ë©´ì ', 'ë§¤ë§¤ê°€', 'ë³´ì¦ê¸ˆ'] if k in sheet_data.columns and k in df_org.columns]
                if len(valid_keys) < 2: return False, "ì‹ë³„ í‚¤ ë¶€ì¡±", None

                # ì„œëª… ìƒì„±
                target_sigs = create_match_signature(df_org.loc[changed_ids].reset_index(), valid_keys)['_match_sig'].tolist()
                server_sigs = create_match_signature(sheet_data, valid_keys)
                
                # ë§¤ì¹­ ë° ì—…ë°ì´íŠ¸
                update_count = 0
                for idx, sig in zip(target_sigs, changed_ids):
                    # ì„œë²„ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì„œëª…ì„ ê°€ì§„ í–‰ ì°¾ê¸°
                    match_indices = server_sigs.index[server_sigs['_match_sig'] == idx].tolist()
                    if match_indices:
                        match_idx = match_indices[0] # ì²« ë²ˆì§¸ ë§¤ì¹­ ì‚¬ìš©
                        for col in sheet_data.columns:
                            if col in df_new.columns: 
                                # ê°’ ì—…ë°ì´íŠ¸ (ìˆ«ì ë³€í™˜ í¬í•¨)
                                val = df_new.loc[sig, col]
                                if col in NUMERIC_COLS:
                                    try: 
                                        val_str = re.sub(r'[^0-9.]', '', str(val))
                                        val = float(val_str) if val_str else 0.0
                                    except: val = 0.0
                                sheet_data.at[match_idx, col] = val
                        update_count += 1
                
                if update_count == 0: return False, "ì›ë³¸ ë°ì´í„° ë§¤ì¹­ ì‹¤íŒ¨ (ì„œë²„ ë°ì´í„°ê°€ ë³€ê²½ë¨)", None
                
                # ë¬´ê²°ì„± ê²€ì¦
                is_valid, msg = validate_data_integrity(sheet_data)
                if not is_valid: return False, f"ë¬´ê²°ì„± ì˜¤ë¥˜: {msg}", None
                
                # ì €ì¥
                conn.update(spreadsheet=SHEET_URL, worksheet=sheet_name, data=sheet_data)
                return True, f"âœ… {update_count}ê±´ ì €ì¥ ì™„ë£Œ!", None
                
            except Exception as e:
                time.sleep(attempt + 1)
                last_err = e
                continue # ì¬ì‹œë„
                
        return False, f"ğŸš¨ ì¬ì‹œë„ ì‹¤íŒ¨: {last_err}", None
        
    except Exception as e: 
        return False, f"ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", traceback.format_exc()

def execute_transaction(action_type, target_rows, source_sheet, target_sheet=None):
    """
    [Phase 2] ì‚­ì œ, ì´ë™, ë³µêµ¬, ë³µì‚¬ íŠ¸ëœì­ì…˜ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        if target_rows.empty: return False, "ëŒ€ìƒ ì—†ìŒ", None
        
        # 1. ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
        src_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=source_sheet, ttl=0))
        target_clean = target_rows.drop(columns=['ì„ íƒ', 'IronID'], errors='ignore')
        
        # 2. ë§¤ì¹­ í‚¤ ì„¤ì •
        v_keys = [k for k in ['ë²ˆì§€', 'ì¸µ', 'ë©´ì ', 'ë³´ì¦ê¸ˆ', 'ë§¤ë§¤ê°€', 'ì›”ì°¨ì„', 'ë‚´ìš©'] if k in src_df.columns and k in target_clean.columns]
        
        # 3. ì„œëª… ìƒì„±
        src_sig = create_match_signature(src_df, v_keys)
        tgt_sig = create_match_signature(target_clean, v_keys)
        sigs = tgt_sig['_match_sig'].tolist()

        # 4. ì•¡ì…˜ ë¶„ê¸°
        if action_type in ["delete", "move", "restore"]:
            # ì†ŒìŠ¤ì—ì„œ ì œê±° (ì„œëª…ì´ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” í–‰ë§Œ ë‚¨ê¹€)
            new_src = src_df[~src_sig['_match_sig'].isin(sigs)]
            
            if len(src_df) == len(new_src): 
                return False, "ë§¤ì¹­ ì‹¤íŒ¨ (ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ë³€ê²½ë¨)", pd.DataFrame({"Target": sigs[:1], "Server": src_sig['_match_sig'].iloc[:1]})
            
            # ì´ë™/ë³µêµ¬ì˜ ê²½ìš° íƒ€ê²Ÿ ì‹œíŠ¸ì— ì¶”ê°€
            if action_type in ["move", "restore"] and target_sheet:
                tgt_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=target_sheet, ttl=0))
                # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸° (ì„ íƒì )
                common_cols = [c for c in src_df.columns if c in tgt_df.columns]
                # ë°ì´í„° ë³‘í•©
                new_tgt = pd.concat([tgt_df, target_clean[common_cols]], ignore_index=True)
                
                # ë¬´ê²°ì„± ê²€ì¦
                is_valid, msg = validate_data_integrity(new_tgt)
                if not is_valid: return False, msg, None
                
                # íƒ€ê²Ÿ ì‹œíŠ¸ ì—…ë°ì´íŠ¸
                conn.update(spreadsheet=SHEET_URL, worksheet=target_sheet, data=new_tgt)
            
            # ì†ŒìŠ¤ ì‹œíŠ¸ ì—…ë°ì´íŠ¸ (ì‚­ì œ ë°˜ì˜)
            conn.update(spreadsheet=SHEET_URL, worksheet=source_sheet, data=new_src)
            return True, f"âœ… {action_type} ì²˜ë¦¬ ì™„ë£Œ", None
        
        elif action_type == "copy":
            # íƒ€ê²Ÿ ì‹œíŠ¸ ë¡œë“œ
            tgt_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=target_sheet, ttl=0))
            
            # ë°ì´í„° ë³‘í•© (ì¤‘ë³µ ê²€ì‚¬ ì—†ì´ ë‹¨ìˆœ ì¶”ê°€ - ìš”ì²­ì‚¬í•­)
            # í•„ìš” ì‹œ ì¤‘ë³µ ê²€ì‚¬ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            common_cols = [c for c in src_df.columns if c in tgt_df.columns]
            new_tgt = pd.concat([tgt_df, target_clean[common_cols]], ignore_index=True)
            
            conn.update(spreadsheet=SHEET_URL, worksheet=target_sheet, data=new_tgt)
            return True, "âœ… ë³µì‚¬ ì™„ë£Œ", None
            
    except Exception as e: 
        return False, str(e), traceback.format_exc()
