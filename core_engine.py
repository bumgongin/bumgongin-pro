# core_engine.py
# ë²”ê³µì¸ Pro v24 Enterprise - Core Data Engine Module (v24.29.2)
# Feature: Negative Floor Regex Fix (Precision Surgery)

import streamlit as st
import pandas as pd
from streamlit_gsheets import GSheetsConnection
import urllib.parse
import time
import uuid
import re
import traceback
import math
from datetime import datetime
import requests
import json

# ==============================================================================
# [SECTION 1: GLOBAL CONFIGURATION]
# ==============================================================================

SHEET_URL = "https://docs.google.com/spreadsheets/d/1bmTnLu-vMvlAGRSsCI4a8lk00U38covWl5Wfn9JZYVU"

SHEET_GIDS = {
    "ì„ëŒ€": "2063575964", "ì„ëŒ€(ì¢…ë£Œ)": "791354475", 
    "ë§¤ë§¤": "1833762712", "ë§¤ë§¤(ì¢…ë£Œ)": "1597438389",
    "ì„ëŒ€ë¸Œë¦¬í•‘": "982780192", "ë§¤ë§¤ë¸Œë¦¬í•‘": "807085458"
}
SHEET_NAMES = list(SHEET_GIDS.keys())

NUMERIC_COLS = ["ë³´ì¦ê¸ˆ", "ì›”ì°¨ì„", "ê¶Œë¦¬ê¸ˆ", "ê´€ë¦¬ë¹„", "ë§¤ë§¤ê°€", "ìˆ˜ìµë¥ ", "ë©´ì ", "ëŒ€ì§€ë©´ì ", "ì—°ë©´ì ", "ì¸µ"]
STRING_COLS = ["êµ¬ë¶„", "ì§€ì—­_êµ¬", "ì§€ì—­_ë™", "ë²ˆì§€", "ê±´ë¬¼ëª…", "ë‚´ìš©", "ë¹„ê³ "]
REQUIRED_COLS = ["ë²ˆì§€"]

# ==============================================================================
# [SECTION 2: DATA SANITIZATION ENGINE]
# ==============================================================================

def normalize_headers(df):
    """
    êµ¬ê¸€ ì‹œíŠ¸ í—¤ë”ë¥¼ í‘œì¤€í™”í•©ë‹ˆë‹¤.
    ë™ì˜ì–´ ì‚¬ì „ì„ í™•ì¥í•˜ì—¬ ì‹¤ë¬´ ìš©ì–´ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
    """
    df.columns = df.columns.str.replace(' ', '').str.strip()
    synonym_map = {
        "ë³´ì¦ê¸ˆ": ["ë³´ì¦ê¸ˆ(ë§Œì›)", "ê¸°ë³´ì¦ê¸ˆ(ë§Œì›)", "ê¸°ë³´ì¦ê¸ˆ", "ë³´ì¦ê¸ˆ", "ë³´ì¦"],
        "ì›”ì°¨ì„": ["ì›”ì°¨ì„(ë§Œì›)", "ê¸°ì›”ì„¸(ë§Œì›)", "ì›”ì„¸(ë§Œì›)", "ì›”ì„¸", "ê¸°ì›”ì„¸", "ì°¨ì„"],
        "ê¶Œë¦¬ê¸ˆ": ["ê¶Œë¦¬ê¸ˆ_ì…ê¸ˆê°€(ë§Œì›)", "ê¶Œë¦¬ê¸ˆ(ë§Œì›)", "ê¶Œë¦¬ê¸ˆ", "ê¶Œë¦¬", "ì‹œì„¤ê¶Œë¦¬"],
        "ê´€ë¦¬ë¹„": ["ê´€ë¦¬ë¹„(ë§Œì›)", "ê´€ë¦¬ë¹„"],
        "ë§¤ë§¤ê°€": ["ë§¤ë§¤ê°€(ë§Œì›)", "ë§¤ë§¤ê¸ˆì•¡(ë§Œì›)", "ë§¤ë§¤ê¸ˆì•¡", "ë§¤ë§¤ê°€", "ë§¤ê°€", "ë§¤ë§¤"],
        "ë©´ì ": ["ì „ìš©ë©´ì (í‰)", "ì‹¤í‰ìˆ˜", "ì „ìš©ë©´ì ", "ë©´ì "],
        "ëŒ€ì§€ë©´ì ": ["ëŒ€ì§€ë©´ì (í‰)", "ëŒ€ì§€", "ëŒ€ì§€ë©´ì "],
        "ì—°ë©´ì ": ["ì—°ë©´ì (í‰)", "ì—°ë©´ì "],
        "ìˆ˜ìµë¥ ": ["ìˆ˜ìµë¥ (%)", "ìˆ˜ìµë¥ "],
        "ì¸µ": ["í•´ë‹¹ì¸µ", "ì¸µ", "ì§€ìƒì¸µ", "ì¸µìˆ˜"],
        "ë‚´ìš©": ["ë§¤ë¬¼íŠ¹ì§•", "íŠ¹ì§•", "ë¹„ê³ ", "ë‚´ìš©"],
        "ë²ˆì§€": ["ì§€ì—­_ë²ˆì§€", "ë²ˆì§€", "ì§€ë²ˆ"],
        "êµ¬ë¶„": ["ë§¤ë¬¼êµ¬ë¶„", "êµ¬ë¶„"],
        "ê±´ë¬¼ëª…": ["ê±´ë¬¼ëª…", "ë¹Œë”©ëª…"]
    }
    for standard, aliases in synonym_map.items():
        for alias in aliases:
            clean_alias = alias.replace(' ', '')
            if clean_alias in df.columns:
                df.rename(columns={clean_alias: standard}, inplace=True)
                break 
    return df

def sanitize_dataframe(df):
    """
    ë°ì´í„°í”„ë ˆì„ ê°’ì„ ì •ì œí•©ë‹ˆë‹¤.
    [v24.29.2 ìˆ˜ìˆ  ì™„ë£Œ] 'ì¸µ' ì»¬ëŸ¼ì€ ë§ˆì´ë„ˆìŠ¤(-) ê¸°í˜¸ë¥¼ ë³´ì¡´í•˜ì—¬ ì§€í•˜ì¸µì„ ì˜¬ë°”ë¥´ê²Œ ì¸ì‹í•©ë‹ˆë‹¤.
    ë‚˜ë¨¸ì§€ ìˆ«ì ì»¬ëŸ¼(ê¸ˆì•¡, ë©´ì  ë“±)ì€ ìŒìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ì¡´ëŒ€ë¡œ ì •ì œí•©ë‹ˆë‹¤.
    """
    for col in NUMERIC_COLS:
        if col in df.columns:
            try:
                val_str = df[col].astype(str)
                
                # [ìˆ˜ì •ëœ ë¡œì§] ì¸µìˆ˜ëŠ” ë§ˆì´ë„ˆìŠ¤ í—ˆìš©, ê·¸ ì™¸ëŠ” ìˆ«ìë§Œ ë‚¨ê¹€
                if col == 'ì¸µ':
                    # ì •ê·œì‹: ìŒìˆ˜ ê¸°í˜¸(-) ì˜µì…˜ í¬í•¨, ìˆ«ì ë° ì†Œìˆ˜ì  ì¶”ì¶œ
                    cleaned_series = val_str.str.extract(r'(-?[\d.]+)')[0]
                    # ì¸µìˆ˜ëŠ” ê²°ì¸¡ ì‹œ 1ì¸µìœ¼ë¡œ ê°€ì •
                    df[col] = pd.to_numeric(cleaned_series, errors='coerce').fillna(1)
                else:
                    # ê¸°ì¡´ ë¡œì§: ìˆ«ì(0-9)ì™€ ì†Œìˆ˜ì (.)ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±° (ìŒìˆ˜ ë¶ˆê°€)
                    cleaned_series = val_str.str.replace(r'[^0-9.]', '', regex=True)
                    # ê¸ˆì•¡/ë©´ì ì€ ê²°ì¸¡ ì‹œ 0ìœ¼ë¡œ ê°€ì •
                    df[col] = pd.to_numeric(cleaned_series, errors='coerce').fillna(0)
            except: 
                df[col] = 0.0
                
    for col in STRING_COLS:
        if col in df.columns:
            try:
                # ë¬¸ìì—´ ì»¬ëŸ¼: ë¶ˆí•„ìš”í•œ ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
                df[col] = df[col].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip()
            except: 
                df[col] = ""
                
    return df.fillna("")

def validate_data_integrity(df):
    """
    í•„ìˆ˜ ì»¬ëŸ¼ ë° ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    errors = []
    for col in REQUIRED_COLS:
        if col not in df.columns: 
            errors.append(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")
        elif df[col].astype(str).str.strip().eq("").any():
            errors.append(f"í•„ìˆ˜ê°’ ëˆ„ë½: {col} ì»¬ëŸ¼ì— ë¹ˆ í–‰ì´ ìˆìŠµë‹ˆë‹¤.")
    
    if errors: 
        return False, "\n".join(errors)
    return True, "Integrity Check Passed"

# ==============================================================================
# [SECTION 3: CORE LOAD ENGINE]
# ==============================================================================

def initialize_search_state():
    """
    ì•± ì‹¤í–‰ ì‹œ ì„¸ì…˜ ìƒíƒœ(ê²€ìƒ‰ í•„í„° ë“±)ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    if 'editor_key_version' not in st.session_state:
        st.session_state.editor_key_version = 0
        
    defaults = {
        'search_keyword': "", 'exact_bunji': "", 'selected_cat': [], 
        'selected_gu': [], 'selected_dong': [], 'is_no_kwon': False,
        'min_price': 0.0, 'max_price': 10000000.0, 'min_dep': 0.0, 'max_dep': 1000000.0,
        'min_rent': 0.0, 'max_rent': 10000.0, 'min_kwon': 0.0, 'max_kwon': 100000.0,
        'min_area': 0.0, 'max_area': 10000.0, 'min_land': 0.0, 'max_land': 10000.0,
        'min_total': 0.0, 'max_total': 10000.0, 'min_fl': -5.0, 'max_fl': 100.0
    }
    for k, v in defaults.items():
        if k not in st.session_state: st.session_state[k] = v

def safe_reset():
    """
    í•„í„° ê´€ë ¨ ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ê³  ë¦¬ëŸ°í•©ë‹ˆë‹¤.
    """
    for key in list(st.session_state.keys()):
        # í•µì‹¬ ì‹œìŠ¤í…œ ë³€ìˆ˜ëŠ” ìœ ì§€
        if key not in ['current_sheet', 'editor_key_version']:
            del st.session_state[key]
    st.session_state.editor_key_version += 1
    st.cache_data.clear()
    st.rerun()

@st.cache_data(ttl=600)
def load_sheet_data(sheet_name):
    """
    êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. (IronID ë³´ì¡´ ë° ë¹ˆì¹¸ ì±„ìš°ê¸°)
    """
    gid = SHEET_GIDS.get(sheet_name)
    if not gid: return None
    
    csv_url = f"{SHEET_URL}/export?format=csv&gid={gid}"
    
    try:
        df = pd.read_csv(csv_url)
        df = normalize_headers(df)
        df = sanitize_dataframe(df)
        
        # [ìˆ˜ì • ì§€ì ] IronID ì—´ì„ í™•ì¸í•˜ê³  ë¹„ì–´ìˆëŠ” ì¹¸(NaN)ì— ê³ ìœ  ë²ˆí˜¸ë¥¼ ì±„ì›ë‹ˆë‹¤.
        if 'IronID' not in df.columns:
            df['IronID'] = [str(uuid.uuid4()) for _ in range(len(df))]
        else:
            # ì‹œíŠ¸ì— ì—´ì€ ìˆì§€ë§Œ ì¹¸ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆì „ì¥ì¹˜
            df['IronID'] = df['IronID'].apply(lambda x: str(uuid.uuid4()) if pd.isna(x) or str(x).strip() == "" else str(x))
        
        if 'ì„ íƒ' in df.columns: df = df.drop(columns=['ì„ íƒ'])
        df.insert(0, 'ì„ íƒ', False)
        
        return df
    except Exception as e:
        print(f"[Load Error] {e}")
        return None

# ==============================================================================
# [SECTION 4: MATCHING ENGINE]
# ==============================================================================

def create_match_signature(df, keys):
    """
    ë°ì´í„° ë§¤ì¹­ì„ ìœ„í•œ ê³ ìœ  ì„œëª…(Signature)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    temp_df = df.copy()
    temp_df['_match_sig'] = ""
    
    for k in keys:
        try:
            if k in NUMERIC_COLS:
                val_str = temp_df[k].astype(str)
                if k == 'ì¸µ':
                    val_str = val_str.str.extract(r'(-?[\d.]+)')[0]
                else:
                    val_str = val_str.str.replace(r'[^0-9.]', '', regex=True)
                
                val = pd.to_numeric(val_str, errors='coerce').fillna(0)
                temp_df['_match_sig'] += val.round(1).astype(str).str.replace(r'\.0$', '', regex=True) + "|"
            else:
                val = temp_df[k].astype(str).str[:20] if k == 'ë‚´ìš©' else temp_df[k].astype(str)
                temp_df['_match_sig'] += val.str.replace(r'[^ê°€-í£a-zA-Z0-9]', '', regex=True) + "|"
        except: continue
        
    return temp_df

# ==============================================================================
# [SECTION 5: UPDATE ENGINE (FULL LOGIC)]
# ==============================================================================

def update_single_row(updated_row, sheet_name):
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        raw_data = conn.read(spreadsheet=SHEET_URL, worksheet=sheet_name, ttl=0)
        sheet_data = normalize_headers(raw_data)
        sheet_data = sanitize_dataframe(sheet_data)

        if 'IronID' not in sheet_data.columns:
            sheet_data['IronID'] = ""

        target_id = updated_row.get('IronID')
        row_idx = None

        match_list = sheet_data.index[sheet_data['IronID'].astype(str) == str(target_id)].tolist()
        
        if match_list:
            row_idx = match_list[0]
        else:
            # ë¹„ìƒ ìˆ˜ìƒ‰: ë²ˆì§€/ì¸µ/ë©´ì ìœ¼ë¡œ ì°¾ê¸° (ìˆ«ì í˜•ì‹ ë¬´ì‹œ ë¡œì§)
            v_keys = [k for k in ['ë²ˆì§€', 'ì¸µ', 'ë©´ì '] if k in sheet_data.columns]
            for idx, row in sheet_data.iterrows():
                is_match = True
                for k in v_keys:
                    try:
                        if float(re.sub(r'[^0-9.-]', '', str(row.get(k, '')))) != float(re.sub(r'[^0-9.-]', '', str(updated_row.get(k, '')))):
                            is_match = False; break
                    except:
                        if str(row.get(k, '')).strip() != str(updated_row.get(k, '')).strip():
                            is_match = False; break
                if is_match: row_idx = idx; break

        if row_idx is None: return False, "âŒ ë§¤ì¹­ ì‹¤íŒ¨: ë°ì´í„° ì¼ì¹˜ í•­ëª© ì—†ìŒ"

        for k, v in updated_row.items():
            if k in sheet_data.columns and k not in ['ì„ íƒ']:
                if k in NUMERIC_COLS:
                    try: v = float(re.sub(r'[^0-9.-]', '', str(v)))
                    except: v = 0.0
                sheet_data.at[row_idx, k] = v
        
        conn.update(spreadsheet=SHEET_URL, worksheet=sheet_name, data=sheet_data)
        return True, "âœ… ì €ì¥ ì„±ê³µ!"
    except Exception as e: return False, f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"

def save_updates_to_sheet(edited_df, original_df, sheet_name):
    """
    [Phase 1] ë¦¬ìŠ¤íŠ¸/ì¹´ë“œ ë·°ì—ì„œì˜ ëŒ€ëŸ‰ ìˆ˜ì • ì‚¬í•­ì„ ë°°ì¹˜ ì €ì¥í•©ë‹ˆë‹¤.
    """
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        # 1. ë³€ê²½ëœ í–‰ ê°ì§€
        df_org = original_df.set_index('IronID')
        df_new = edited_df.set_index('IronID')
        
        changed_ids = []
        for iid in df_org.index.intersection(df_new.index):
            row_org = df_org.loc[iid].drop(['ì„ íƒ'], errors='ignore').astype(str)
            row_new = df_new.loc[iid].drop(['ì„ íƒ'], errors='ignore').astype(str)
            if not row_org.equals(row_new):
                changed_ids.append(iid)
        
        if not changed_ids: return True, "ë³€ê²½ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.", None

        # 2. ì¬ì‹œë„ ë¡œì§
        for attempt in range(3):
            try:
                sheet_data = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=sheet_name, ttl=0))
                valid_keys = [k for k in ['ë²ˆì§€', 'ì¸µ', 'ë©´ì ', 'ë§¤ë§¤ê°€', 'ë³´ì¦ê¸ˆ'] if k in sheet_data.columns and k in df_org.columns]
                if len(valid_keys) < 2: return False, "ì‹ë³„ í‚¤ ë¶€ì¡±", None

                target_sigs = create_match_signature(df_org.loc[changed_ids].reset_index(), valid_keys)['_match_sig'].tolist()
                server_sigs = create_match_signature(sheet_data, valid_keys)
                
                update_count = 0
                for idx, sig in zip(target_sigs, changed_ids):
                    match_indices = server_sigs.index[server_sigs['_match_sig'] == idx].tolist()
                    if match_indices:
                        match_idx = match_indices[0]
                        for col in sheet_data.columns:
                            if col in df_new.columns: 
                                val = df_new.loc[sig, col]
                                if col in NUMERIC_COLS:
                                    try: 
                                        val_str = str(val)
                                        if col == 'ì¸µ':
                                            match = re.search(r'(-?[\d.]+)', val_str)
                                            val_clean = match.group(1) if match else "0"
                                        else:
                                            val_clean = re.sub(r'[^0-9.]', '', val_str)
                                        val = float(val_clean) if val_clean else 0.0
                                    except: val = 0.0
                                sheet_data.at[match_idx, col] = val
                        update_count += 1
                
                if update_count == 0: return False, "ì›ë³¸ ë°ì´í„° ë§¤ì¹­ ì‹¤íŒ¨", None
                
                is_valid, msg = validate_data_integrity(sheet_data)
                if not is_valid: return False, f"ë¬´ê²°ì„± ì˜¤ë¥˜: {msg}", None
                
                conn.update(spreadsheet=SHEET_URL, worksheet=sheet_name, data=sheet_data)
                return True, f"âœ… {update_count}ê±´ ì €ì¥ ì™„ë£Œ!", None
                
            except Exception as e:
                time.sleep(attempt + 1)
                last_err = e
                continue
                
        return False, f"ğŸš¨ ì¬ì‹œë„ ì‹¤íŒ¨: {last_err}", None
        
    except Exception as e: 
        return False, f"ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", traceback.format_exc()

def execute_transaction(action_type, target_rows, source_sheet, target_sheet=None):
    """
    [Phase 2] ì‚­ì œ, ì´ë™, ë³µêµ¬, ë³µì‚¬ íŠ¸ëœì­ì…˜ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    conn = st.connection("gsheets", type=GSheetsConnection)
    try:
        if target_rows.empty: return False, "ëŒ€ìƒ ì—†ìŒ", None
        
        # 1. ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
        src_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=source_sheet, ttl=0))
        target_clean = target_rows.drop(columns=['ì„ íƒ', 'IronID'], errors='ignore')
        
        # 2. ë§¤ì¹­ í‚¤ ì„¤ì •
        v_keys = [k for k in ['ë²ˆì§€', 'ì¸µ', 'ë©´ì ', 'ë³´ì¦ê¸ˆ', 'ë§¤ë§¤ê°€', 'ì›”ì°¨ì„', 'ë‚´ìš©'] if k in src_df.columns and k in target_clean.columns]
        
        # 3. ì„œëª… ìƒì„±
        src_sig = create_match_signature(src_df, v_keys)
        tgt_sig = create_match_signature(target_clean, v_keys)
        sigs = tgt_sig['_match_sig'].tolist()

        # 4. ì•¡ì…˜ ë¶„ê¸°
        if action_type in ["delete", "move", "restore"]:
            # ì†ŒìŠ¤ì—ì„œ ì œê±°
            new_src = src_df[~src_sig['_match_sig'].isin(sigs)]
            
            if len(src_df) == len(new_src): 
                return False, "ë§¤ì¹­ ì‹¤íŒ¨ (ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ë³€ê²½ë¨)", pd.DataFrame({"Target": sigs[:1], "Server": src_sig['_match_sig'].iloc[:1]})
            
            # ì´ë™/ë³µêµ¬: íƒ€ê²Ÿ ì‹œíŠ¸ì— ì¶”ê°€
            if action_type in ["move", "restore"] and target_sheet:
                tgt_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=target_sheet, ttl=0))
                common_cols = [c for c in src_df.columns if c in tgt_df.columns]
                new_tgt = pd.concat([tgt_df, target_clean[common_cols]], ignore_index=True)
                
                is_valid, msg = validate_data_integrity(new_tgt)
                if not is_valid: return False, msg, None
                
                conn.update(spreadsheet=SHEET_URL, worksheet=target_sheet, data=new_tgt)
            
            # ì†ŒìŠ¤ ì‹œíŠ¸ ì—…ë°ì´íŠ¸
            conn.update(spreadsheet=SHEET_URL, worksheet=source_sheet, data=new_src)
            return True, f"âœ… {action_type} ì²˜ë¦¬ ì™„ë£Œ", None
        
        elif action_type == "copy":
            tgt_df = normalize_headers(conn.read(spreadsheet=SHEET_URL, worksheet=target_sheet, ttl=0))
            common_cols = [c for c in src_df.columns if c in tgt_df.columns]
            new_tgt = pd.concat([tgt_df, target_clean[common_cols]], ignore_index=True)
            
            conn.update(spreadsheet=SHEET_URL, worksheet=target_sheet, data=new_tgt)
            return True, "âœ… ë³µì‚¬ ì™„ë£Œ", None
            
    except Exception as e: 
        return False, str(e), traceback.format_exc()





